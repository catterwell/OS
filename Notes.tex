\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{parskip}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{centernot}
\usepackage{hyperref}
\usepackage{eufrak}
\usepackage{graphicx}
\graphicspath{{pics/}}
\geometry{a4paper, left=20mm, right=20mm, top=20mm, bottom=20mm}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Anthony Catterwell}
\chead{\textsc{University of Edinburgh}}
\rhead{Operating Systems}

\title{Operating Systems Lecture Notes}
\author{Anthony Catterwell}

\begin{document}
\maketitle
\tableofcontents

\break{}

\section{Introduction}

\section{Operating System Structure}

\subsection{Architectural impact}

\textbf{Architectural features affecting OS\emph{s}}
\begin{itemize}
    \item These features were built primarily to support OS\emph{s}:
        \begin{itemize}
            \item timer (clock) operationg
            \item synchronisation instructions
            \item memory protection
            \item I/O control operations
            \item interrupts and exceptions
            \item protected modes of operation (kernel vs.\ user mode)
            \item privileged instructions
            \item system calls (including software interrupts)
            \item virtualisation architectures
        \end{itemize}
    \item ASPLOS
\end{itemize}

\subsection{User operating interaction}

\subsubsection{User v.s.\ kernel}

\textbf{Privileged instructions}
\begin{itemize}
    \item Some instructions are restricted to the OS
        \begin{itemize}
            \item known as \emph{privileged} instructions
        \end{itemize}
    \item Only the OS can:
        \begin{itemize}
            \item directly access I/O devices
            \item manipulate memory state management (page table pointers, TLB loads, etc.)
            \item manipulate special \emph{mode bits} (interrupt priority level)
        \end{itemize}
    \item Restrictions provide safety and security
\end{itemize}

\textbf{OS protections}
\begin{itemize}
    \item So how does the process know if a privileged instruction should be executed?
        \begin{itemize}
            \item the architecture must support at least two modes of operation:
                kernel mode, and user mode
            \item mode is set by status bit in a protected processor register.
                \begin{itemize}
                    \item user programs execute in user mode
                    \item OS executes in kernel (privileged) mode (OS == kernel)
                \end{itemize}
            \item Privileged instructions can only be executed in kernel (privileged) mode
                \begin{itemize}
                    \item if code running in user mode attempts to execute a privileged
                        instruction, the illegal execution trap.
                \end{itemize}
        \end{itemize}
\end{itemize}

\textbf{Crossing protection boundaries}
\begin{itemize}
    \item So how do user programs do something privileged?
        \begin{itemize}
            \item e.g.\ how can you write to a disk if you can't execute any I/O instructions?
        \end{itemize}
    \item User programs must call on OS procedure --- that is to ask the OS to do it for them.
        \begin{itemize}
            \item OS defines a set of system calls
            \item User-mode program executes system call instruction
        \end{itemize}
    \item Syscall instruction
        \begin{itemize}
            \item like a protected procedure call
        \end{itemize}
\end{itemize}

\subsubsection{Syscall}

\textbf{Syscall}
\begin{itemize}
    \item The syscall instruction \emph{atomically}:
        \begin{itemize}
            \item saves the current PC
            \item sets the execution mode to privileged
            \item sets the PC to a handler address
        \end{itemize}
    \item Similar to a procedure call
        \begin{itemize}
            \item Caller puts arguments in a place the callee expects (registers, or stack)
                \begin{itemize}
                    \item One of the args is a syscall number, indicating which OS function
                        to invoke
                \end{itemize}
            \item Callee (OS) saves caller's state (registers, other control states) so it can
                use the CPU
            \item OS function code runs
                \begin{itemize}
                    \item OS must verify caller's arguments (e.g.\ pointers)
                \end{itemize}
            \item OS returns using a special instruction
                \begin{itemize}
                    \item Automatically sets PC to return address and sets execution mode to
                        user.
                \end{itemize}
        \end{itemize}
\end{itemize}

\begin{center}{}
    \includegraphics[height=270]{a-kernel-crossing-illustrated.jpg}
\end{center}

\textbf{System call issues}
\begin{itemize}
    \item A syscall is not a subroutine call, with the caller specifying the next PC.\
        \begin{itemize}
            \item the caller knows where the subroutines are located in memory;
                therefore they can be the target of an attack.
        \end{itemize}
    \item The kernel saves state?
        \begin{itemize}
            \item Prevents overwriting of values
        \end{itemize}
    \item The kernel verify arguments
        \begin{itemize}
            \item Prevents buggy code crashing the system
        \end{itemize}
    \item Referring to kernel objects as arguments
        \begin{itemize}
            \item Data copied between user buffer and kernel buffer.
        \end{itemize}
\end{itemize}

\textbf{Exception handling and protection}
\begin{itemize}
    \item \emph{All} entries to the OS occur via the mechanism just shown
        \begin{itemize}
            \item Acquiring privileged mode and branching to the trap handler are inseparable
        \end{itemize}
    \item Terminology
        \begin{itemize}
            \item \emph{Interrupt}: asynchronous; caused by an external device
            \item \emph{Exception}: synchronous; unexpected problem with instruction
            \item \emph{Trap}: synchronous; intended transition to OS due to an instruction
        \end{itemize}
        In all three cases, they are instances of where something strange happens,
        and the OS takes control: whether by accident, or by intention.
    \item Privileged instructions and resources are the basis for most everything:
        memory protection, protected I/O, limiting user resource consumption.
\end{itemize}

\subsection{Operating System structure}

\subsubsection{Layers}

\textbf{Operating System structure}
\begin{itemize}
    \item The OS sits between application programs and the hardware
        \begin{itemize}
            \item it mediates access and abstracts away ugliness
            \item programs request services via traps or exceptions
            \item devices request attention via interrupts
        \end{itemize}
\end{itemize}

\textbf{Operating system design and implementation}
\begin{itemize}
    \item Design and implementation of OS not ``solvable'', but some approaches have proven
        successful.
    \item Internal structure of different OS\emph{s} can vary widely.
    \item Start the design by defining goals and specifications.
    \item Affected by choice of hardware, type of system.
    \item \emph{User} goals, and \emph{system} goals
        \begin{itemize}
            \item User goals: OS should be convenient to use, easy to learn, reliable, safe,
                and fast
            \item System goals: OS should be easy to design, implement, and maintain,
                as well as flexible, reliable, error-free, and efficient.
        \end{itemize}
    \item Important principle to separate
        \begin{itemize}
            \item \textbf{Policy}: \emph{What} will be done?
            \item \textbf{Mechanism}: \emph{How} to do it?
        \end{itemize}
    \item Mechanisms determine how to do something, policies decide what will be done.
    \item The separation of policy from mechanism is a very important principle,
        it allows maximum flexibility if policy decisions are to be changed later
        (e.g.\ timer).
    \item Specifying and designing an OS is a highly creative task of
        \emph{software engineering}.
\end{itemize}

\textbf{System layers}
\begin{center}{}
    \includegraphics[height=270]{system-layers.jpg}
\end{center}

\textbf{Major OS components}
\begin{itemize}
    \item processes
    \item memory
    \item I/O
    \item secondary storage
    \item file systems
    \item protection
    \item shells
    \item GUI
    \item networking
\end{itemize}

\textbf{OS structure}

\begin{itemize}
    \item There's no clear hierarchy within an OS --- each of them needs access to different
        things.
    \item An OS consists of all these components, plus:
        \begin{itemize}
            \item many other components
            \item system programs (privileged, and non-privileged)
        \end{itemize}
    \item Major issue:
        \begin{itemize}
            \item how do we organize all this?
            \item what are all of the code modules, and where do they exist?
            \item how do they cooperate?
        \end{itemize}
    \item Massive software engineering and design problem
        \begin{itemize}
            \item design a large, complex program that:
                performs well, is reliable, is extensible, and is backwards compatible.
        \end{itemize}
\end{itemize}

\subsubsection{Examples}

\textbf{Monolithic design}
\begin{itemize}
    \item Traditionally, OS\emph{s} (like UNIX) were built as a \emph{monolithic} entity
        User programs | OS (everything) | hardware
    \item Major advantage: cost of module interactions is low (procedure call)
    \item Disadvantages:
        \begin{itemize}
            \item hard to understand
            \item hard to modify
            \item unreliable (no isolation between system modules)
            \item hard to maintain
        \end{itemize}
    \item What is the alternative? \\
        Find a way to organise the OS in order to simplify its design and implementation.
\end{itemize}

\textbf{Layering}
\begin{itemize}
    \item The traditional approach is layering
        \begin{itemize}
            \item implement OS as a set of layers
            \item each layer presents an enhanced \emph{virtual machine} to the layer above
        \end{itemize}
    \item The first description of this approach was Dijkstra's THE system
        \begin{itemize}
            \item Layer 5: \emph{Job managers} execute users' programs
            \item Layer 4: \emph{Device managers} handle devices and provide buffering
            \item Layer 3: \emph{Console manager} implements virtual consoles
            \item Layer 2: \emph{Page manager} implements virtual memories for each process
            \item Layer 1: \emph{Kernel} implements a virtual processor for each process
            \item Layer 0: \emph{Hardware}
        \end{itemize}
    \item Each layer can be tested and verified independently
    \item Imposes a hierarchical stricture
        \begin{itemize}
            \item but real systems are more complex:
                file systems require VM services (buffer);
                VM would like to use files for its backing store
            \item strict layering isn't flexible enough
        \end{itemize}
    \item Poor performance:
        each layer crossing has \emph{overhead} associated with it
    \item Disjunction between model and reality:
        systems modelled as layers, but not really built that way.
\end{itemize}

\textbf{Hardware abstraction layer}
\begin{itemize}
    \item An example of layering in modern operating systems
    \item Goal: separates hardware-specific routines from the \emph{core} OS
        \begin{itemize}
            \item Provides portability
            \item Improves readability
        \end{itemize}
\end{itemize}

\textbf{Microkernels}
\begin{itemize}
    \item Popular in the late 80s, early 90s
    \item Goal:
        minimize what happens in kernel;
        item organize rest of OS as user-level processes.
    \item This results in:
        \begin{itemize}
            \item better reliability (isolation between components)
            \item easy of extension and customisation
            \item poor performance (user/kernel boundary crossings)
        \end{itemize}
    \item First microkernel system was Hydra (CMU, 1970)
        \begin{itemize}
            \item Contemporaries: Mach (CMU), Chorus (French UNIX-like OS), OS X (Apple),
                in some ways NT (Microsoft)
        \end{itemize}
\end{itemize}

\begin{center}{}
    \includegraphics[height=270]{microkernel-structure-illustrated.jpg}
\end{center}

\textbf{Comparison of OS structures}

Windows

\includegraphics[height=270]{windows.jpg}

MINIX 3

\includegraphics[height=270]{minix-3.jpg}

\textbf{Loadable kernel modules}

\begin{itemize}
    \item (Perhaps) the best practice for OS design
    \item Core services in the kernel, and others dynamically loaded
    \item Common implementations include: Solaris, Linux, etc.
    \item Advantages
        \begin{itemize}
            \item convenient: no need for rebooting for newly added modules
            \item efficient: no need for message passing unlike micro-kernel
            \item flexible: any module can call any other module unlike layered model
        \end{itemize}
\end{itemize}

\subsection{Summary}

\begin{itemize}
    \item Fundamental distinction between user and privileged mode supported by most hardware
    \item OS design has been an evolutionary process of trial and error.
    \item Successful OS designs have run the spectrum  from monolithic, to layered,
        to micro-kernels
    \item The role and design of an OS are still evolving
    \item It is impossible to pick one ``correct'' way to structure an OS
\end{itemize}

\break{}

\section{Processes}

\subsection{Process}

\textbf{What is a ``process''?}
\begin{itemize}
    \item The process is the OS\emph{s} abstraction for execution
        \begin{itemize}
            \item A process is a program in execution
        \end{itemize}
    \item Simplest (classic) case: a \emph{sequential process}
        \begin{itemize}
            \item An address space (an abstraction of memory)
            \item A single thread of execution (an abstraction of the CPU)
        \end{itemize}
    \item A sequential process is:
        \begin{itemize}
            \item The unit of execution
            \item The unit of scheduling
            \item The dynamic (active) execution context
                (as opposed to the program --- static, just a bunch of bytes)
        \end{itemize}

\end{itemize}

\textbf{What's ``in'' a process?}
\begin{itemize}
    \item A process consists of (at least):
        \begin{itemize}
            \item An \emph{address space}, containing:
                \begin{itemize}
                    \item the code (instructions) for the running program
                    \item the data for the running program (static data, heap data, stack)
                \end{itemize}
            \item \emph{CPU state}, consisting of:
                \begin{itemize}
                    \item the program counter (PC), indicating the next instruction;
                    \item the stack pointer;
                    \item other general purpose register values.
                \end{itemize}
            \item A set of \emph{OS resources}
                \begin{itemize}
                    \item open files, network connections, sound channels, \dots
                \end{itemize}
            \item In other words, everything needed to run the program
                (or to restart, if interrupted).
        \end{itemize}
\end{itemize}

\includegraphics[height=270]{a-process-address-space.jpg}

\textbf{The OS process namespace}
\begin{itemize}
    \item The particulars depend on the specific OS, but the principles are general;
    \item The name for a process is called a \emph{process ID} (PID) (an integer);
    \item The PID namespace is global to the system;
    \item Operations that create processes return a PID (e.g.\ fork);
    \item Operations on processes take PIDs as an argument (e.g.\ kill, wait, nice).\
\end{itemize}

\subsection{Process control block}

\textbf{Representation of processes by the OS}
\begin{itemize}
    \item The OS maintains a data structure to keep track of a process's state
        \begin{itemize}
            \item called the \emph{process control block} (PCB)
                or \emph{process descriptor};
            \item identified by the PID.\
        \end{itemize}
    \item OS keeps all of a process's execution state in (or linked from) the PCB
        when the process isn't running
        \begin{itemize}
            \item PC, SP, registers, etc.
            \item when a process is unscheduled, the state is transferred out of the
                hardware into the PCB
            \item (when a process is running, its state is spread between the PCB and the CPU).
        \end{itemize}
\end{itemize}

\textbf{The PCB}
\begin{itemize}
    \item The PCB is a data structure with many, many fields
        \begin{itemize}
            \item PID
            \item parent PID
            \item execution state
            \item PC, SP, registers
            \item address space info
            \item UNIX user id, group id
            \item scheduling priority
            \item accounting info
            \item pointers for state queues
        \end{itemize}
    \item In Linux:
        \begin{itemize}
            \item defined in \texttt{task\_struct (include/linux/sched.h)}
            \item Over 95 fields!
        \end{itemize}
\end{itemize}

\subsection{Process state \& context switch}

\textbf{PCBs and CPU state}
\begin{itemize}
    \item When a process is running, its CPU state is inside the CPU
        \begin{itemize}
            \item PC, SP, registers
            \item CPU contains current values
        \end{itemize}
    \item When the OS gets control because of a
        \begin{itemize}
            \item \emph{Trap}: program executes a syscall
            \item \emph{Exception}: program does something unexpected (e.g.\ page fault)
            \item \emph{Interrupt}: A hardware device requests service
        \end{itemize}
        the OS saves the CPU state of the running process in that process's PCB.\

    \item When the OS returns the process to the running state
        \begin{itemize}
            \item it loads the hardware registers with values from that process's PCB
            \item e.g.\ general purpose registers, SP, instruction pointer
        \end{itemize}
    \item This act of switching the CPU from one process to another is called a
        \emph{context switch}
        \begin{itemize}
            \item systems may do 100s or 1000s of switches per second;
            \item takes a few microseconds on today's hardware;
            \item still expensive relative to thread-based context switches.\
        \end{itemize}
    \item Choosing which process to run next is called \emph{scheduling}.\
\end{itemize}

\includegraphics[height=270]{process-context-switch.jpg}

\textbf{Process execution states}
\begin{itemize}
    \item Each process has an \emph{execution state}, which indicates what it's currently doing
        \begin{itemize}
            \item \emph{ready}: waiting to be assigned to a CPU ---
                could run, but another process has the CPU;\
            \item \emph{running}: executing on a CPU ---
                it's the process that currently controls the CPU;\
            \item \emph{waiting} (aka ``blocked''): waiting for an event, e.g.\ I/O completion,
                or a messing from (or the completion of) another process ---
                cannot make progress until the event happens.
        \end{itemize}
    \item As a process executes, it moves from state to state
        \begin{itemize}
            \item UNIX:\ run \texttt{top}, STAT column shows current state
            \item which state is a process most of the time?
        \end{itemize}
\end{itemize}

\includegraphics[height=270]{process-states-and-state-transitions.jpg}

\textbf{State queues}
\begin{itemize}
    \item The OS maintains a collection of queues that represent the state of all processes in
        the system
        \begin{itemize}
            \item typically one queue for each state (e.g.\ ready, waiting, \dots);
            \item each PCB is queued onto a state queue according to the current state of the
                process it represents;
            \item as a process changes state, its PCB is unlinked from one queue,
                and linked onto another.
        \end{itemize}
    \item The PCBs are moved between queues, which are represented as linked lists.
    \item There may be many wait queues, one for each type of wait
        (particular device, timer, message, \dots).
\end{itemize}

\textbf{PCBs and state queues}
\begin{itemize}
    \item PCBs are data structures
        \begin{itemize}
            \item dynamically allocated inside OS memory.
        \end{itemize}
    \item When a process is created:
        \begin{itemize}
            \item OS allocates a PCB for it;
            \item OS initializes PCB;\
            \item (OS does other things not related to the PCB);
            \item OS puts PCB on the correct queue.
        \end{itemize}
    \item As a process computes:
        \begin{itemize}
            \item OS moves its PCB from queue to queue.
        \end{itemize}
    \item When a process is terminated:
        \begin{itemize}
            \item PCB may be retained for a while (to receive signals, etc.)
            \item eventually, OS deallocates the PCB.\
        \end{itemize}
\end{itemize}

\subsection{Process creation and termination}

\textbf{Process creation}
\begin{itemize}
    \item New processes are created by existing processes
        \begin{itemize}
            \item creator is called the \emph{parent};
            \item created process is called the \emph{child};\\\
                UNIX:\ do \texttt{ps -ef}, look for PPID field
            \item what creates the first process, and when? \\
                on UNIX, this first process is init; \\
                on many Linux distributions, this is SystemD or Runit (on Void).
        \end{itemize}
\end{itemize}

\textbf{Process creation semantics}
\begin{itemize}
    \item (Depending on the OS) child processes inherit certain attributes of the parent.
        E.g.
        \begin{itemize}
            \item Open file table: implies \texttt{stdin}/\texttt{stdout}/\texttt{stderr};
            \item On some systems, resource allocation to parent may be divided among children.
        \end{itemize}
    \item (In Unix) when a child is created, the parent may either wait for the child to
        finish, or continue in parallel.
\end{itemize}

\textbf{UNIX process creation details}
\begin{itemize}
    \item UNIX process creation through \texttt{fork} system call
        \begin{itemize}
            \item creates and initializes a new PCB
                \begin{itemize}
                    \item initializes kernel resources of new process with resources of parent
                        (e.g.\ open files)
                    \item initializes PC, SP to be same as parent.
                \end{itemize}
            \item creates a new address space
                \begin{itemize}
                    \item initialises new address space with a copy of the entire contents of
                        the address space of the parent
                \end{itemize}
            \item places new PCB on the ready queue.
        \end{itemize}
    \item the \texttt{fork} system call ``returns twice''
        \begin{itemize}
            \item once into the parent, and once into the child
                \begin{itemize}
                    \item returns the child's PID to the parent
                    \item returns \texttt{0} to the child
                \end{itemize}
        \end{itemize}
    \item \texttt{fork} = ``clone me''. \\
        The return value is used to determine whether we're the clone or the original.
\end{itemize}

\includegraphics[height=270]{fork.jpg}

\textbf{\texttt{exec} v.s.\ \texttt{fork}}
\begin{itemize}
    \item Q:\ So how do we start a new program, instead of just forking the old program?
    \item A:\ First \texttt{fork}, then \texttt{exec}.
    \item \texttt{exec}
        \begin{itemize}
            \item stops the current process
            \item loads program `prog' into the address space
                (i.e.\ overwrites the existing process image)
            \item initialises hardware context, args for new program
            \item places PCB onto ready queue
            \item \emph{does not create a new process!}
        \end{itemize}
\end{itemize}

\includegraphics[height=270]{exec-and-fork.jpg}

\textbf{Method 1: \texttt{vfork}}
\begin{itemize}
    \item \texttt{vfork} is the older (now uncommon) of the two approaches.
    \item Instead of ``child's address space is a copy of the parent's'',
        the semantics are ``child's address space \emph{is} the parent's'',
        \begin{itemize}
            \item with a ``promise'' that the child won't modify the address space before doing
                an \texttt{execve}.
            \item When \texttt{execve} is called, a new address space is created and it's
                loaded with the new executable.
            \item Parent is blocked until \texttt{execve} is executed by child.
            \item Saves wasted effort of duplicating parent's address space.
        \end{itemize}
\end{itemize}

\textbf{Method 2: copy-on-write}
\begin{itemize}
    \item Retains the original semantics, but copies ``only what is necessary'' rather than
        the entire address space.
    \item On \texttt{fork}:
        \begin{itemize}
            \item Create a new address space
            \item Initialise page tables with same mappings as the parent's
                (i.e.\ they both point to the same physical memory).
                \begin{itemize}
                    \item (No copying of address space contents have occurred at this point
                        --- with the sole exception of the top page of the stack.)
                \end{itemize}
            \item Set both parent and child page tables to make all pages read-only
            \item If either parent or child writes to memory, an exception occurs.
            \item When exception occurs, OS copies the page, adjusts page tables, etc.
        \end{itemize}
\end{itemize}

\subsection{Summary}
\begin{itemize}
    \item Process
    \item PCB
    \item Process state
    \item Context switch
    \item Process creation and termination
\end{itemize}

\break{}

\section{Threads}

\subsection{Process vs Threads}

\textbf{What's \emph{in} a process?}
\begin{itemize}
    \item A process consists of (at least):
        \begin{itemize}
            \item An \emph{address space}, containing
                \begin{itemize}
                    \item the code (instructions) for the running program
                    \item the data for the running program
                \end{itemize}
            \item \emph{Thread state}, consisting of
                \begin{itemize}
                    \item The PC, indicating the next instruction
                    \item The SP, indicating the position on the stack
                    \item Other general purpose registers
                \end{itemize}
            \item A set of \emph{OS resources}
                \begin{itemize}
                    \item Open files, network connections, sound channels, \dots
                \end{itemize}
        \end{itemize}
    \item Decompose \dots
        \begin{itemize}
            \item address space
            \item \emph{thread of control} (stack, SP, PC, registers)
            \item OS resources
        \end{itemize}
\end{itemize}

\textbf{Motivation}
\begin{itemize}
    \item Threads are about \emph{concurrency} and \emph{parallelism}
    \item One way to get concurrency and parallelism is to use multiple processes
        \begin{itemize}
            \item The programs (code) of distinct processes are isolated from each other
        \end{itemize}
    \item Threads are another way to get concurrency and parallelism
        \begin{itemize}
            \item Threads \emph{share a process} --- same address space, same OS resources
            \item Threads have private stack, CPU state --- are schedulable
        \end{itemize}
\end{itemize}

\textbf{What's needed?}
\begin{itemize}
    \item In many cases
        \begin{itemize}
            \item Everybody wants to run the same code
            \item Everybody wants to access the same data
            \item Everybody has the same privileges
            \item Everybody uses the same resources (open files, network connections, etc.)
        \end{itemize}
    \item But you'd like to have multiple hardware execution states:
        \begin{itemize}
            \item an execution stack and SP
                \begin{itemize}
                    \item traces state of procedure calls made
                \end{itemize}
            \item the PC, indicating the next instruction
            \item a set of general-purpose processor registers and their values
        \end{itemize}
\end{itemize}

\textbf{How could we achieve this?}
\begin{itemize}
    \item Given the process abstraction as we know it:
        \begin{itemize}
            \item for several processes
            \item cause each to \emph{map} to the \emph{same} physical memory to share data
                (\texttt{shmget}),
        \end{itemize}
    \item This is really inefficient
        \begin{itemize}
            \item space: PCB, page tables, etc.
            \item time: creating OS structures, fork/copy address space, etc.
        \end{itemize}
\end{itemize}

\textbf{Can we do better?}
\begin{itemize}
    \item Key idea:
        \begin{itemize}
            \item separate the concept of a \emph{process} (address space, OS resources)
            \item \dots from that of a minimal \emph{thread of control}
                (execution state: stack, SP, PC, registers),
        \end{itemize}
    \item This execution state is usually called a \emph{thread}, or a
        \emph{lightweight process}.
\end{itemize}

\textbf{Threads and processes}
\begin{itemize}
    \item Most modern OS\emph{s} support two entities:
        \begin{itemize}
            \item the \emph{process}, which defines the address space and general process
                attributes (such as open files, etc.)
            \item the \emph{thread}, which defines a sequential execution stream within a
                process.
        \end{itemize}
    \item A thread is bound to a single process / address space
        \begin{itemize}
            \item address spaces, however, can have multiple threads executing within them
            \item sharing data between threads is cheap: all see the same address space
            \item creating threads is cheap, too!
        \end{itemize}
    \item \emph{Threads become the unit of scheduling}
        \begin{itemize}
            \item processes / address spaces are just \emph{containers} in which threads
                execute.
        \end{itemize}
\end{itemize}

\textbf{Single and Multi-threaded Processes}
\begin{itemize}
    \item Different threads in the same process have separate registers and stacks.
    \item This is cheaper than duplicating the instructions and PCB etc.,
        as required by having multiple processes.
\end{itemize}

\subsection{Concurrency}

\textbf{Communication}
\begin{itemize}
    \item Threads are concurrent executions sharing an address space (and some OS resources)
    \item Address spaces provide isolation
        \begin{itemize}
            \item If you can't name an object, you can't read or write to it
        \end{itemize}
    \item Hence, communicating between processes is expensive
        \begin{itemize}
            \item Must go through the OS to move data from one address space to another
        \end{itemize}
    \item Because threads are in the same address space, communication is simple/cheap
        \begin{itemize}
            \item Just update a shared variable!
        \end{itemize}
\end{itemize}

\textbf{The design space}

\includegraphics[height=270]{the-design-space.jpg}

\textbf{Process address space}

\includegraphics[height=270]{process-address-space.jpg}

\includegraphics[height=270]{process-address-space-threads.jpg}

\subsection{Design space of process/threads}

\textbf{Process/thread separation}
\begin{itemize}
    \item Concurrency (multi-threading) is useful for:
        \begin{itemize}
            \item handling concurrent events (e.g.\ web servers and clients)
            \item building parallel programs (e.g.\ matrix multiply, ray tracing)
            \item improving program structure (the Java argument),
        \end{itemize}
    \item Multi-threading is useful even on a uniprocessor
        \begin{itemize}
            \item even though only one thread can run at a time
        \end{itemize}
    \item Supporting multi-threading --- that is, separating the concept of a \emph{process}
        (address space, files, etc.) from that of a minimal \emph{thread of control}
        (execution state), is a big win
        \begin{itemize}
            \item creating concurrency does not require creating new processes
            \item ``faster / better / cheaper''
        \end{itemize}
\end{itemize}

\subsection{Kernel threads}

\textbf{Where do threads come from?}
\begin{itemize}
    \item Natural answer: the OS is responsible for creating/managing threads \\
        For example, the kernel call to create a new thread would
        \begin{itemize}
            \item allocate an execution stack within the process address space
            \item create and initialize a \emph{Thread Control block} \\
                (SP, PC, register values)
            \item stick it on the ready queue
        \end{itemize}
    \item We call these \emph{kernel threads} \\
        There is a ``thread name space''
        \begin{itemize}
            \item Thread IDs (TIDs)
            \item TIDs are integers
        \end{itemize}
\end{itemize}

\includegraphics[height=270]{kernel-threads.jpg}

\textbf{Kernel Threads}
\begin{itemize}
    \item OS now manages threads \emph{and} processes / address spaces
        \begin{itemize}
            \item all thread operations are implemented in the kernel
            \item OS schedules all of the threads in a system
                \begin{itemize}
                    \item if one thread in a process blocks (e.g.\ on I/O),
                        the OS knows about it, and can run other threads from that process
                    \item possible to overlap I/O and computation \emph{inside} a process
                \end{itemize}
        \end{itemize}
    \item Kernel threads are cheaper than processes
        \begin{itemize}
            \item less state to allocate and initialise
        \end{itemize}
    \item But, they're still pretty expensive for fine-grained use
        \begin{itemize}
            \item orders of magnitude more expensive than a procedure call
            \item thread operations are all \emph{system calls}
                \begin{itemize}
                    \item context switch
                    \item argument checks
                \end{itemize}
            \item must maintain kernel state for each thread
        \end{itemize}
\end{itemize}

\subsection{User-level threads}

\textbf{Cheaper alternative}
\begin{itemize}
    \item There is an alternative to kernel threads
    \item Threads can also be managed at the user level (within the process)
        \begin{itemize}
            \item a library linked into the program manages the threads
                \begin{itemize}
                    \item the thread manager doesn't need to manipulate address spaces
                        (which only the kernel can do)
                    \item threads differ (roughly) only in hardware contexts
                        (PC, SP, registers), which can be manipulated by user-level code
                    \item the \emph{thread package} multiplexes user-level threads on top
                        of kernel threads
                    \item each kernel thread is treated as a \emph{virtual processor}
                \end{itemize}
            \item we call these \emph{user-level threads}
        \end{itemize}
\end{itemize}

\includegraphics[height=270]{user-level-threads.jpg}

\textbf{User-level threads}
\begin{itemize}
    \item User-level threads are small and fast
        \begin{itemize}
            \item managed entirely by user-level library (e.g.\ \texttt{pthreads})
            \item each thread is represented by a PC, registers, a stack, and a small
                \emph{thread control block} (TCB)
            \item creating a thread, switching between threads, and synchronising
                threads are done \emph{via procedure calls}
                \begin{itemize}
                    \item no kernel involvement necessary!
                \end{itemize}
        \end{itemize}
    \item User-level thread operations can be 10--100x faster than kernel threads as a result.
\end{itemize}

\textbf{User-level thread implementation}
\begin{itemize}
    \item The OS schedules the kernel thread
    \item The kernel thread executes user code, including the thread support library
        and its associated thread scheduler
    \item The thread scheduler determines when a user-level thread runs
        \begin{itemize}
            \item it uses queues to keep track of what threads are doing: run, ready, wait
                \begin{itemize}
                    \item just like the OS and processes
                    \item but, implemented at user-level as a library
                \end{itemize}
        \end{itemize}
\end{itemize}

\textbf{Thread context switch}
\begin{itemize}
    \item Very simple for user-level threads:
        \begin{itemize}
            \item save context of currently running thread
                \begin{itemize}
                    \item push CPU state onto thread stack
                \end{itemize}
            \item restore context of the next thread
                \begin{itemize}
                    \item pop CPU state from next thread's stack
                \end{itemize}
            \item return as the new thread
                \begin{itemize}
                    \item execution resume at PC of next thread
                \end{itemize}
            \item Note: no changes to memory mapping required
        \end{itemize}
    \item This is all done in assembly language
        \begin{itemize}
            \item it works at the level of the procedure calling convention
        \end{itemize}
\end{itemize}

\textbf{How to keep a user-level thread from hogging the CPU?}
\begin{itemize}
    \item Strategy 1: force everyone to cooperate
        \begin{itemize}
            \item a thread willingly gives up the CPU by calling \texttt{yield}
            \item \texttt{yield} calls into the scheduler, which context switches to
                another ready thread
            \item what happens if a thread never calls \texttt{yield}?
        \end{itemize}
    \item Strategy 2: use presumption
        \begin{itemize}
            \item scheduler requests that a timer interrupt be delivered by the OS
                periodically
                \begin{itemize}
                    \item usually delivered as a UNIX signal (\texttt{man signal})
                    \item signals are just like software interrupts, but delivered to
                        user-level by the OS instead of delivered to the OS by hardware
                \end{itemize}
            \item at each timer interrupt, scheduler gains control and context switches
                as appropriate.
        \end{itemize}
\end{itemize}

\textbf{What if a thread tries to do I/O}
\begin{itemize}
    \item The kernel thread ``powering'' it is lost for the duration of (synchronous)
        I/O operation!
        \begin{itemize}
            \item The kernel thread blocks in the OS, as always
            \item It maroons with it the state of the user-level thread
        \end{itemize}
    \item Could have one kernel thread ``powering'' each user-level thread
        \begin{itemize}
            \item ``common case'' operations (e.g.\ synchronisation) would be quick
        \end{itemize}
    \item Could have a limited-size ``pool'' of kernel threads ``powering'' all the
        user-level threads in the address space
        \begin{itemize}
            \item the kernel will be scheduling these threads, obliviously to what's going
                on at user-level.
        \end{itemize}
\end{itemize}

\subsection{Summary}
\begin{itemize}
    \item Multiple threads per address space
    \item Kernel threads are much more efficient than processes, but still expensive
        \begin{itemize}
            \item all operations require a kernel call and parameter validation
        \end{itemize}
    \item User-level threads are:
        \begin{itemize}
            \item much cheaper and faster
            \item great for common-case operations
                \begin{itemize}
                    \item creation, synchronisation, destruction
                \end{itemize}
            \item can suffer in uncommon cases due to kernel obliviousness
                \begin{itemize}
                    \item I/O
                    \item pre-emption of a lock-holder
                \end{itemize}
        \end{itemize}
\end{itemize}

\section{Synchronisation}

\textbf{Temporal relations}
\begin{itemize}
    \item User view of parallel threads
        \begin{itemize}
            \item Instructions executed by a single thread are totally ordered
                \begin{itemize}
                    \item $A < B < C < \dots$
                \end{itemize}
            \item In absence of \emph{synchronisation}:
                \begin{itemize}
                    \item instructions executed by distinct threads must be considered
                        unordered / simultaneous
                    \item Not $X < X'$, and not $X' < X$
                \end{itemize}
        \end{itemize}
    \item Hardware largely supports this
\end{itemize}

\textbf{Critical sections / mutual exclusion}
\begin{itemize}
    \item Sequences of instructions that may get incorrect results if executed simultaneously
        are called \emph{critical sections}.
    \item \emph{Race condition} results depend on timing
    \item \emph{Mutual exclusion} means ``not simultaneously''
        \begin{itemize}
            \item $A < B$ or $B < A$
            \item We don't care which
        \end{itemize}
    \item Forcing mutual exclusion between two critical section executions
        \begin{itemize}
            \item is sufficient to ensure correct execution
            \item guarantees ordering.
        \end{itemize}
\end{itemize}

\includegraphics[height=270]{critical-sections.jpg}

\textbf{When do critical sections arise?}
\begin{itemize}
    \item One common pattern:
        \begin{itemize}
            \item read-modify-write of
            \item a shared value (variable)
            \item in code that can be executed by concurrent threads
        \end{itemize}
    \item Shared variable:
        \begin{itemize}
            \item Global and heap-allocated variables
            \item NOT local variables (which are on the stack)
        \end{itemize}
\end{itemize}

\textbf{Race conditions}
\begin{itemize}
    \item A program has a \emph{race condition} (data race) if the result of an
        execution depends on timing (i.e.\ it is non-deterministic)
    \item Typical symptoms
        \begin{itemize}
            \item I run it on the same data, and sometimes it prints 0 and sometimes 4
            \item I run it on the same data, and sometimes it prints 0 and sometimes crashes
        \end{itemize}
\end{itemize}

\textbf{Correct critical section requirements}
\begin{itemize}
    \item \emph{Mutual exclusion} \\
        At most one thread is in the critical section.
    \item \emph{Progress} \\
        If thread $T$ is outside the critical section,
        then $T$ cannot prevent thread $S$ from entering the critical section.
    \item \emph{Bounded waiting} (no \emph{starvation}) \\
        If thread $T$ is waiting on the critical section,
        then $T$ will eventually enter the critical section
        (assumes threads eventually leave critical sections).
    \item \emph{Performance} \\
        The overhead of entering and exiting the critical section is small with respect to
        the work being done within it.
\end{itemize}

\textbf{Mechanisms for building critical sections}
\begin{itemize}
    \item Spinlocks
        \begin{itemize}
            \item primitive, minimal semantics --- used to build others
        \end{itemize}
    \item Semaphores (and non-spinning locks)
        \begin{itemize}
            \item basic, easy to understand, somewhat hard to program with
        \end{itemize}
    \item Monitors
        \begin{itemize}
            \item higher level, requires language support, implicit operations
            \item easier to program with; Java ``\texttt{synchronised}'', for example
        \end{itemize}
    \item Messages
        \begin{itemize}
            \item Simple model of communication and synchronisation based on (atomic)
                transfer of data across a channel
            \item direct application to distributed systems
        \end{itemize}
\end{itemize}

\subsection{Locks}

\textbf{Locks}
\begin{itemize}
    \item A lock is a memory object with two operations:
        \begin{itemize}
            \item \texttt{acquire}: obtain the right to enter the critical section
            \item \texttt{release}: give up the right to be in the critical section
        \end{itemize}
    \item \texttt{acquire} \emph{prevents the progress of the thread until the lock
        can be acquired}.
    \item Note: terminology varies: acquire/release, lock/unlock
\end{itemize}

\includegraphics[height=270]{locks.jpg}

\textbf{Acquire/release}
\begin{itemize}
    \item Threads pair up calls to \texttt{acquire} and \texttt{release}
        \begin{itemize}
            \item between \texttt{acquire} and \texttt{release}, the thread \emph{holds}
                the lock
            \item \texttt{acquire} does not return until the caller ``owns'' (holds) the lock
                \begin{itemize}
                    \item at most one thread can hold a lock at a time
                \end{itemize}
        \end{itemize}
    \item What happens if the calls aren't paired
        \begin{itemize}
            \item I acquire, but neglect to release?
        \end{itemize}
    \item What happens if the two threads acquire different locks
        \begin{itemize}
            \item I think that access to a particular shared data structure is
                mediated by lock A, and you think it's mediated by lock B?\
        \end{itemize}
    \item What is the right granularity of locking?
\end{itemize}

\subsection{Spinlocks}

\textbf{Spinlocks}
\begin{itemize}
    \item How do we implement spinlocks?
        Here's one attempt:
        \begin{verbatim}
    struct lock_t {
        int held = 0;
    }
    void acquire(lock) {
        while (lock->held);
        lock->held = 1;
    }
    void release(lock) {
        lock->held = 0;
    }
        \end{verbatim}
    \item Race condition in acquire.
\end{itemize}

\textbf{Implementing spinlocks}
\begin{itemize}
    \item Problem is that implementation of spinlocks has critical sections, too!
        \begin{itemize}
            \item the acquire/release must be \emph{atomic}
            \item compiler can hoist code that is invariant
        \end{itemize}
    \item Need help from the hardware
        \begin{itemize}
            \item atomic instructions \\
                test-and-set, compare-and-swap, \dots
        \end{itemize}
\end{itemize}

\textbf{Spinlocks: Hardware Test-and-Set}
\begin{itemize}
    \item CPU provides the following as \emph{one atomic instruction}:
        \begin{verbatim}
    bool test_and_set(bool *flag) {
        bool old = *flag;
        *flag = True;
        return old;
    }
        \end{verbatim}
    \item This is a single \emph{atomic} instruction
\end{itemize}

\textbf{Implementing spinlocks using Test-and-Set}
\begin{itemize}
    \item So, to fix our broken spinlocks:
        \begin{verbatim}
    struct lock{
        int held = 0;
    }
    void acquire(lock) {
        while (test_and_set(&lock->held));
    }
    void release(lock) {
        lock->held = 0;
    }
    \end{verbatim}
\item \emph{mutual exclusion?} (at most one thread in the critical section)
\item \emph{progress?} (\texttt{T} outside cannot prevent \texttt{S} from entering)
\item \emph{bounded waiting?} (waiting \texttt{T} will eventually enter)
\item \emph{performance?} (low overhead (modulo the spinning part\dots))
\end{itemize}

\section{Semaphores, Condition Variables, and Monitors}

\subsection{Semaphore}

\textbf{Semaphore}
\begin{itemize}
    \item More sophisticated synchronisation mechanism
    \item Semaphore \texttt{S} --- integer variable
    \item Can only be accessed via two atomic operations: \texttt{wait} and \texttt{signal}
        (originally called \texttt{P} and \texttt{V}).
    \item Definitions
        \begin{verbatim}
    wait(S) {
        while (S <= 0); // busy wait
        S--;
    }
    signal(S) {
        S++;
    }
        \end{verbatim}
    \item These are performed \emph{atomically}
\end{itemize}

\textbf{Semaphore Usage}
\begin{itemize}
    \item \emph{Counting semaphore}: integer value can range over an unrestricted domain
    \item \emph{Binary semaphore}: integer value can range only between 0 and 1
        (same as \emph{lock})
    \item Can solve various synchronisation problems
    \item Consider $\texttt{P}_1$ and $\texttt{P}_2$ that require $\texttt{S}_1$ to
        happen before $\texttt{S}_2$ \\
        Create a semaphore ``\texttt{synch}'' initialised to 0
        \begin{verbatim}
    P1:
        S_1;
        signal(synch);
    P2:
        wait(synch);
        S_2;
        \end{verbatim}
    \item Can implement a counting semaphore \texttt{S} as a binary semaphore.
\end{itemize}

\textbf{Implementation with no Busy waiting}

Each semaphore has an associated queue of threads
\begin{verbatim}
    wait(semaphore *S) {
        S->value--;
        if (S->value < 0) {
            add this thread to S->list;
            block();
        }
    }
    signal(semaphore *S) {
        S->value++;
        if (S->value <= 0) {
            remove a thread T from S->list;
            wakeup(T);
        }
    }
\end{verbatim}

\underline{Examples}

\includegraphics[height=270]{bounded-buffer-using-semaphores.jpg}

\includegraphics[height=270]{readers-writers-using-semaphores.jpg}

\textbf{Semaphores v.s.\ Spinlocks}
\begin{itemize}
    \item Threads that are blocked at the level of program logic
        (that is, by the semaphore \texttt{P} operation)
        are placed on queues, rather than busy-waiting.
    \item Busy-waiting may be used for the ``real'' mutual exclusion required to implement
        \texttt{P} and \texttt{V}
        \begin{itemize}
            \item but these are very short critical sections ---
                totally independent of program logic
            \item and they are not implemented by the application programmer.
        \end{itemize}
\end{itemize}

\textbf{Abstract implementation}
\begin{itemize}
    \item \texttt{P\,(sem)}
        \begin{itemize}
            \item acquire ``real'' mutual exclusion
                \begin{itemize}
                    \item if \texttt{sem} is ``available'' (> 0), decrement sum;
                        \emph{release ``real'' mutual exclusion};
                        let thread continue
                    \item otherwise,  place thread on associated queue;
                        \emph{release ``real'' mutual exclusion};
                        run some other thread.
                \end{itemize}
        \end{itemize}
\item \texttt{V\,(sem)}
    \begin{itemize}
        \item \emph{acquire ``real'' mutual exclusion}
            \begin{itemize}
                \item if threads are waiting on the associated queue,
                    unblock one (place it on the ready queue)
                \item if no threads are on the queue, \texttt{sem} is incremented \\
                    the signal is ``remembered'' for the next time \texttt{P\,(sem)} is called
            \end{itemize}
        \item release ``real'' mutual exclusion
        \item the ``\texttt{V}-ing'' thread continues execution.
    \end{itemize}
\end{itemize}

\textbf{Problems with semaphores, locks}
\begin{itemize}
    \item They can be used to solve any of the traditional synchronisation problems,
        but it's easy to make mistakes
        \begin{itemize}
            \item they are essentially shared global variables
                \begin{itemize}
                    \item can be accessed from anywhere (bad software engineering)
                \end{itemize}
            \item there is no connection between the synchronisation variable and the
                data being controlled by it
            \item no control over their use, no guarantee of proper usage
                \begin{itemize}
                    \item Semaphores: will here ever be a \texttt{V\,()}?
                    \item Locks: did you lock when necessary?
                        Unlock at the right time?
                        At all?
                \end{itemize}
        \end{itemize}
    \item Thus, they are prone to bugs
        \begin{itemize}
            \item We can reduce the chance of bugs by ``stylising'' the use of synchronisation
            \item Language help is useful for this.
        \end{itemize}
\end{itemize}

\subsection{Monitors}

\textbf{Monitors}
\begin{itemize}
    \item A \underline{programming language construct} supports controlled shared data access
        \begin{itemize}
            \item synchronisation code is added by the compiler.
        \end{itemize}
    \item A class in which every method automatically acquires a lock on entry,
        and releases it on exit --- it combines:
        \begin{itemize}
            \item \emph{shared data} structures (object);
            \item \emph{procedures} that operate on the shared data (object methods);
            \item \emph{synchronisation} between concurrent threads that invoke those
                procedures.
        \end{itemize}
    \item Data can only be accessed from within the monitor
        \begin{itemize}
            \item protects the data from unstructured access;
            \item prevents ambiguity about what the synchronisation variable protects.
        \end{itemize}
    \item Addresses the key usability issues that arise with semaphores.
\end{itemize}

\includegraphics[height=270]{a-monitor.jpg}

\textbf{Monitor facilities}
\begin{itemize}
    \item ``Automatic'' mutual exclusion
        \begin{itemize}
            \item only one thread can be executing inside at any time
                \begin{itemize}
                    \item thus, synchronisation is implicitly associated with the monitor
                        --- it ``comes for free'';
                \end{itemize}
            \item if a second thread tries to execute a monitor procedure,
                it blocks until the first has left the monitor;
                \begin{itemize}
                    \item more restrictive than semaphores,
                    \item but easier to use (most of the time).
                \end{itemize}
        \end{itemize}
    \item But, there's a problem\dots \\
        Bounded buffer scenario.
\end{itemize}

\textbf{Bounded Buffer scenario}
\begin{itemize}
    \item Monitors require condition variables
    \item Operations on condition varibales
        \begin{itemize}
            \item \texttt{wait\,(c)}
                \begin{itemize}
                    \item release monitor lock, so somebody else can get in
                    \item wait for somebody else to signal condition
                    \item thus, condition variables have associated wait queues
                \end{itemize}
            \item \texttt{signal\,(c)}
                \begin{itemize}
                    \item wake up at most one waiting thread
                        \begin{itemize}
                            \item ``Hoare'' monitor: wakeup immediately, signaller steps
                                outside
                        \end{itemize}
                    \item if no waiting threads, signal is lost
                        \begin{itemize}
                            \item this is different from semaphores --- no history!
                        \end{itemize}
                \end{itemize}
            \item \texttt{broadcast\,(c)}
                \begin{itemize}
                        \item wake up all waiting threads.
                \end{itemize}
        \end{itemize}
\end{itemize}

\textbf{Bounded buffer using (Hoare) monitors}
\begin{verbatim}
    Monitor bounded_buffer {
        buffer resources[];
        condition not_full;
        condition not_empty;

        produce(resource x) {
            if (array "resources" is full, determined maybe by a count) {
                wait(not_full);
            }
            insert "x" in array "resources";
            signal(not_empty);
        }

        consume(resource *x) {
            if (array "resources" is empty, determined maybe by a count) {
                wait(not_empty);
            }
            *x = get resource from array "resources";
            signal(not_full);
        }
    }
\end{verbatim}

\textbf{Runtime system calls for (Hoare) monitors}
\begin{itemize}
    \item \texttt{EnterMonitor\,(m)} \{guarantee mutual exclusion\}
    \item \texttt{ExitMonitor\,(m)} \{hit the road, letting someone else run\}
    \item \texttt{Wait\,(c)} \{step out until condition satisfied\}
    \item \texttt{Signal\,(c)} \{if someone's waiting, step out and let them run\}
    \item \texttt{EnterMonitor} and \texttt{ExitMonitor} are inserted automatically by the
        compiler.
    \item This guarantees mutual exclusion for code inside of the monitor.
\end{itemize}

\textbf{Monitor Summary}
\begin{itemize}
    \item Language supports monitors
    \item Compiler understands them
        \begin{itemize}
            \item Compiler inserts calls to runtime routines for
                \begin{itemize}
                    \item monitor entry
                    \item monitor exit
                \end{itemize}
            \item Programmer inserts calls to runtime routines for
                \begin{itemize}
                    \item signal
                    \item wait
                \end{itemize}
            \item Language/object encapsulation ensures correctness
                \begin{itemize}
                    \item Sometimes!
                        With conditions, you \emph{still} need to think about synchronisation
                \end{itemize}
        \end{itemize}
    \item Runtime system implements these routines
        \begin{itemize}
            \item moves threads on and off queues
            \item \emph{ensures mutual exclusion!}
        \end{itemize}
\end{itemize}

\section{Deadlock}

\textbf{Definition}
\begin{itemize}
    \item A thread is deadlocked when it's waiting for an event that can never occur
    \item Thread \texttt{A} is in critical section 1 \\
        waiting for access to critical section 2;
    \item Thread \texttt{B} is in critical section 2 \\ waiting for access to critical section 1
\end{itemize}

\textbf{Four conditions must exist for deadlock to be possible}
\begin{enumerate}
    \item Mutual exclusion
    \item Hold and wait
    \item No pre-emption
    \item Circular wait
\end{enumerate}

\includegraphics[height=270]{deadlock.jpg}

\subsection{Graph reduction}

\textbf{Graph reduction}
\begin{itemize}
    \item A graph can be \emph{reduced} by a thread if all of that thread's requests
        can be granted
        \begin{itemize}
            \item in this case, the thread eventually will terminate --- all resources are
                freed --- all arcs (allocations) to/from it in the graph are deleted.
        \end{itemize}
    \item Miscellaneous theorems (Holt, Havender):
        \begin{itemize}
            \item There are no deadlocked threads if and only if the graph is completely
                reducible.
            \item The order of reductions is irrelevant.
        \end{itemize}
\end{itemize}

\includegraphics[height=270]{resource-allocation-graph-with-a-deadlock.jpg}

\textbf{Handling deadlock}
\begin{itemize}
    \item Eliminate one of the four required conditions
        \begin{itemize}
            \item Mutual exclusion
            \item Hold and Wait
            \item No pre-emption
            \item Circular wait
        \end{itemize}
    \item Broadly classified as:
        \begin{itemize}
            \item Prevention, or
            \item Avoidance, or
            \item Detection (and recovery)
        \end{itemize}
\end{itemize}

\textbf{Deadlock prevention} \\
Restrain the ways requests can be made
\begin{itemize}
    \item Mutual exclusion \\
        not required for sharable resources (e.g.\ read-only files);
        must hold for non-sharable resources.
    \item Hold and wait \\
        must guarantee that whenever a process requests a resources,
        it does not hold any other resources.
        \begin{itemize}
            \item Low resources utilisation; starvation is possible.
        \end{itemize}
    \item No (resource) Pre-emption
        \begin{itemize}
            \item If a process holding some resources requests another unavailable resource
                all resources currently held are released.
            \item Process will be restarted only when it can regain its old resources,
                as well as the new ones that it is requesting.
        \end{itemize}
    \item Circular wait
        \begin{itemize}
            \item impose a total ordering of all resource types, and require that each process
                requests resources in an increasing order of enumeration.
        \end{itemize}
\end{itemize}

\textbf{Avoidance} \\
Less severe restrictions on program behaviour.
\begin{itemize}
    \item Eliminating circular wait
        \begin{itemize}
            \item each thread states its maximum claim for every resource type;
            \item system runs the Banker's Algorithm at each allocation request \\
                Banker $\implies$ highly conservative
        \end{itemize}
\end{itemize}

\subsection{Banker's Algorithm}

\textbf{Banker's Algorithm example}
\begin{itemize}
    \item Background
        \begin{itemize}
            \item The set of controlled resources is known to the system.
            \item The number of units of each resource is known to the system.
            \item Each application must declare its maximum possible requirement of each
                resource type.
        \end{itemize}
    \item The, the system can do the following:
        \begin{itemize}
            \item When a request is made:
                \begin{itemize}
                    \item pretend you granted it;
                    \item pretend all other legal requests were made;
                    \item can the graph be reduced?
                        \begin{itemize}
                            \item If so: allocate the requested resource.
                            \item If not, block the thread until some thread releases resources,
                                and then try pretending again.
                        \end{itemize}
                \end{itemize}
        \end{itemize}
\end{itemize}

\textbf{Safe state}
\begin{itemize}
    \item When requesting an available resource decide if allocation leaves the system in a
        safe state
    \item We're in a \emph{safe state} if there exists a sequence $<P_1, P_2, \ldots, P_n>$
        of \emph{all} the processes in the systems
        \begin{itemize}
            \item such that for each $P_i$, the resources that $P_i$ can still request can be
                satisfied by currently available resources + resources held by all the $P_j$,
                with $j < i$.
        \end{itemize}
    \item That is:
        \begin{itemize}
            \item If $P_i$ resource needs are not immediately available, then $P_i$
                can wait until all $P_j$ have finished.
            \item When $P_j$ is finished, $P_i$ can obtain needed resources, execute, return
                allocated resources, and terminate.
            \item When $P_i$ terminates, $P_{i+1}$ can obtain its needed resources, and so on.
        \end{itemize}
\end{itemize}
Safe $\implies$ no deadlock; \quad deadlock $\implies$ unsafe

\textbf{Data Structures for the Banker's Algorithm}
Let \texttt{n} = number of processes, and \texttt{m} = number of resource types.
\begin{itemize}
    \item \texttt{Available}: Vector of length \texttt{m}.
        If \texttt{Available[j]} = \texttt{k}, there are \texttt{k} instances of resource type
    $\texttt{R}_\texttt{j}$ available.
    \item \texttt{Max} $\texttt{n} \times \texttt{m}$ matrix.
        If \texttt{Allocation[i,j]} = \texttt{k}, then $P_i$ is currently allocated
        \texttt{k} instances of $\texttt{R}_\texttt{j}$
    \item \texttt{Allocation}: $\texttt{n} \times \texttt{m}$ matrix.
        If \texttt{Need[i,j]} = \texttt{k}, then $\texttt{P}_\texttt{i}$ may need \texttt{k}
        more instances of $\texttt{R}_\texttt{j}$ to complete its task.
\end{itemize}
\[
    \texttt{Need[i,j]} = \texttt{Max[i,j]} - \texttt{Allocation[i,j]}
\]

\textbf{Safety Algorithm}
\begin{enumerate}
    \item Let \texttt{Work} and \texttt{Finish} be vectors of length \texttt{m} and \texttt{n},
        respectively.
        Initialise: \\
    \texttt{Work = Available} \\
    \texttt{Finish[i] = false for i = 0..n-1}
\item Find an \texttt{i} such that both:
    \begin{enumerate}
        \item \texttt{Finish[i] == false}
        \item $\texttt{Need}_\texttt{i} \texttt{ <= Work}$
    \end{enumerate}
    If no such \texttt{i} exists, go to step 4
\item \texttt{Work = Work + Allocation} \\
    \texttt{Finish[i] = true} \\
    go to step 2
\item If \texttt{Finish[i] == true, for all i,}
    then the system is in a safe state.
\end{enumerate}

\textbf{Resource-Request Algorithm for Process $\texttt{P}_\texttt{i}$} \\
$\texttt{Request}_\texttt{i}$ = request vector for process $\texttt{P}_\texttt{i}$.
If $\texttt{Request}_\texttt{i}\texttt{[j] == k}$
then process $\texttt{P}_\texttt{i}$ wants \texttt{k} instances of resource type
$\texttt{R}_\texttt{j}$.
\begin{enumerate}
    \item If $\texttt{Request}_\texttt{i} \texttt{ <= Need}_\texttt{i}$ go to step 2.
        Otherwise raise error condition, since process has exceeded its maximum claim.
    \item If $\texttt{Request}_\texttt{i} \texttt{ <= Available}$, go to step 3.
        Otherwise $\texttt{P}_\texttt{i}$ must wait, since resources are not available.
    \item Pretend to allocate requested resources to $\texttt{P}_\texttt{i}$ by modifying
        the state as follows: \\
        $\texttt{Available = Available - Request}$ \\
        $\texttt{Allocation}_\texttt{i} \texttt{ =
        Allocation}_\texttt{i} \texttt{ + Request}_\texttt{i}$ \\
        $\texttt{Need}_\texttt{i} \texttt{ = Need}_\texttt{i} \texttt{ - Request}_\texttt{i}$
        \begin{enumerate}
            \item If safe, then resources allocated to $\texttt{P}_\texttt{i}$
            \item If unsafe, then $\texttt{P}_\texttt{i}$ must wait,
                and the old resource-allocation state is restored.
        \end{enumerate}
\end{enumerate}

\textbf{Deadlock Detection}
\begin{enumerate}
    \item Allow system to enter deadlock state
    \item Detection algorithm
    \item Recovery scheme
\end{enumerate}

\textbf{Single instance of each resource type}
\begin{itemize}
    \item Maintain a \emph{wait-for} graph
        \begin{itemize}
            \item Nodes are processes
            \item $\texttt{P}_\texttt{i} \to \texttt{P}_\texttt{j}$ if $\texttt{P}_\texttt{i}$
                is waiting for $\texttt{P}_\texttt{j}$
        \end{itemize}
    \item Periodically invoke an algorithm that searches for a cycle in the graph.
        \begin{itemize}
            \item If there is a cycle, there exists a deadlock.
        \end{itemize}
    \item An algorithm to detect a cycle in a graph
        \begin{itemize}
            \item has runtime complexity $\mathcal{O}(n^2)$ with $n$ being the number of
                vertices in the graph.
        \end{itemize}
\end{itemize}

\includegraphics[height=270]{resource-allocation-graph-and-wait-for-graph.jpg}

\textbf{Detection-Algorithm usage}
\begin{itemize}
    \item When, and how often to invoke depends on:
        \begin{itemize}
            \item How often a deadlock is likely to occur?
            \item How many processes will need to be rolled back?
                \begin{itemize}
                    \item One for each disjoint cycle.
                \end{itemize}
        \end{itemize}
    \item If detection algorithm is invoked arbitrarily:
        \begin{itemize}
            \item there may be many cycles in the resource graph
            \item we would not be able to tell which deadlocked processes ``caused'' the
                deadlock.
        \end{itemize}
\end{itemize}

\textbf{Recovery from deadlock}
\begin{itemize}
    \item Process termination
        \begin{itemize}
            \item Abort all deadlocked processes
            \item Abort one process at a time until the deadlock cycle is eliminated
            \item In which order should we choose to abort?
        \end{itemize}
    \item Resource pre-emption
        \begin{itemize}
            \item \emph{Select a victim} --- minimise cost
            \item \emph{Rollback} --- return to some safe state,
                restart process for that state.
            \item \emph{Starvation} --- same process may always be picked as victim,
                include number of rollback in cost factor.
        \end{itemize}
\end{itemize}

\textbf{Summary}
\begin{itemize}
    \item Deadlock is bad!
    \item We can deal with it either statically (prevention)
        or dynamically (avoidance and/or detection)
    \item In practice, you'll encounter lock ordering, periodic deadlock detection/correction,
        and minefields.
\end{itemize}

\end{document}
